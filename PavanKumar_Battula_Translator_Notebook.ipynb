{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"138Z3DtBv9HnKPraWyY4pwwwRcdw8clqz","timestamp":1713401039737}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"099b7136b2854d45ad187a816128b362":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aad877fde22d4c08a4c2c9fa50804632","IPY_MODEL_a353bed2d24c4e7c9e64c5525f2f3f77","IPY_MODEL_98c8ff5eaa5749ecb23ee72a31cbc99e"],"layout":"IPY_MODEL_c505e3d30c70412ba85f4a72ba83cada"}},"aad877fde22d4c08a4c2c9fa50804632":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_545057a15ec3430d979489085ba1054c","placeholder":"​","style":"IPY_MODEL_440188a6ff1242b5af73d52a78f8df30","value":"tokenizer_config.json: 100%"}},"a353bed2d24c4e7c9e64c5525f2f3f77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c21f2b98956b4a54abaf43c31c526c38","max":42,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb71a238c94c4249af4836bac9e22428","value":42}},"98c8ff5eaa5749ecb23ee72a31cbc99e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1186273f45b42e4be7150fcbf612925","placeholder":"​","style":"IPY_MODEL_0311eb2a2fd740e2bed1c657e4025921","value":" 42.0/42.0 [00:00&lt;00:00, 2.16kB/s]"}},"c505e3d30c70412ba85f4a72ba83cada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545057a15ec3430d979489085ba1054c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"440188a6ff1242b5af73d52a78f8df30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c21f2b98956b4a54abaf43c31c526c38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb71a238c94c4249af4836bac9e22428":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1186273f45b42e4be7150fcbf612925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0311eb2a2fd740e2bed1c657e4025921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47bbeb11ea774c33b174860f6f9405e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_104f8075ece242eea62bbec2f2104244","IPY_MODEL_d02ca3f5200042fbb452cc7eda7aa4ea","IPY_MODEL_2c16e3bdbe804ef48174a2cc2643cb80"],"layout":"IPY_MODEL_84dbb785b5de48899bab94f99bd6e4b1"}},"104f8075ece242eea62bbec2f2104244":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7a48076a6cf4d61bad8ef8a59fea1cd","placeholder":"​","style":"IPY_MODEL_3bc7aaf8648a4214b3ac68e9f9e579bb","value":"config.json: 100%"}},"d02ca3f5200042fbb452cc7eda7aa4ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44a28cd7579045de9219ed1deec93430","max":1335,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb50c59e9c6d41ab90c660c57f376a44","value":1335}},"2c16e3bdbe804ef48174a2cc2643cb80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64757bde8c3241e495ad1ceb000e4047","placeholder":"​","style":"IPY_MODEL_80b85eee1f04460bb6d6492806ed87c2","value":" 1.33k/1.33k [00:00&lt;00:00, 35.4kB/s]"}},"84dbb785b5de48899bab94f99bd6e4b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a48076a6cf4d61bad8ef8a59fea1cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bc7aaf8648a4214b3ac68e9f9e579bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44a28cd7579045de9219ed1deec93430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb50c59e9c6d41ab90c660c57f376a44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64757bde8c3241e495ad1ceb000e4047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80b85eee1f04460bb6d6492806ed87c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35bb014fa8684dcb997c9f4e54776a51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c85a2626012047dcb09f37cf39308f5d","IPY_MODEL_e850e77136cb4af5ab0b3db779bfd7eb","IPY_MODEL_0619bd2933684e8e9d596068492d60f6"],"layout":"IPY_MODEL_c54155ecd6e44beba644ecbddd735565"}},"c85a2626012047dcb09f37cf39308f5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95ec91753c964ba5b23c20e345db115a","placeholder":"​","style":"IPY_MODEL_55dbd5c87cd84d368ab1f3e393993b52","value":"source.spm: 100%"}},"e850e77136cb4af5ab0b3db779bfd7eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f284be904f4b24bbe34b911746d6c4","max":768489,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fb6bc3282d3454c8e2c171ea10601ad","value":768489}},"0619bd2933684e8e9d596068492d60f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72f5773b4c86454f84fc2f94cc4b15e5","placeholder":"​","style":"IPY_MODEL_8b986f79fda04c469ff9c31b37f358d5","value":" 768k/768k [00:00&lt;00:00, 8.52MB/s]"}},"c54155ecd6e44beba644ecbddd735565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95ec91753c964ba5b23c20e345db115a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55dbd5c87cd84d368ab1f3e393993b52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52f284be904f4b24bbe34b911746d6c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fb6bc3282d3454c8e2c171ea10601ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72f5773b4c86454f84fc2f94cc4b15e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b986f79fda04c469ff9c31b37f358d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6d0871939a7467f8982f555f9b7b32a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e206bba1cc4048a3a5989162cbdac97c","IPY_MODEL_8e1abf4611d54974979f80b792089894","IPY_MODEL_b0f7486da08444b1a9771a3e99bd55e1"],"layout":"IPY_MODEL_8039cea9967c48948e964c3e726738dc"}},"e206bba1cc4048a3a5989162cbdac97c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72690da867544b6d968d9e9a0bdd6f15","placeholder":"​","style":"IPY_MODEL_c7b197ff3265460baad8f08a430fb451","value":"target.spm: 100%"}},"8e1abf4611d54974979f80b792089894":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a199e90bc50843b0903d600a9480b2e1","max":796845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7284cf3e6a424ecebb6beb46040fd38a","value":796845}},"b0f7486da08444b1a9771a3e99bd55e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c694dc7c6ce4375973fd1c93c7f5ad0","placeholder":"​","style":"IPY_MODEL_3b97da80d52a45adaccf7b12fd4f7151","value":" 797k/797k [00:00&lt;00:00, 16.3MB/s]"}},"8039cea9967c48948e964c3e726738dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72690da867544b6d968d9e9a0bdd6f15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7b197ff3265460baad8f08a430fb451":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a199e90bc50843b0903d600a9480b2e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7284cf3e6a424ecebb6beb46040fd38a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c694dc7c6ce4375973fd1c93c7f5ad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b97da80d52a45adaccf7b12fd4f7151":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f468c3f84b6e4c2a91df36e72bb81eb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_088f72f0c35b43f9834b29d48bf631dc","IPY_MODEL_53c6bcee810b48eeb8291c1e717b44ab","IPY_MODEL_86abd68de01346b6bfde05efa8b89a38"],"layout":"IPY_MODEL_41ef1b39759f4c788f2526eae6deabc0"}},"088f72f0c35b43f9834b29d48bf631dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_984f6a7bf00e46d0ae32f3d13360c0cd","placeholder":"​","style":"IPY_MODEL_e3c15884dd7f47949b7e9607e0b3d63a","value":"vocab.json: 100%"}},"53c6bcee810b48eeb8291c1e717b44ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa1bfaf9104444a99c8ebdc5073151d2","max":1273232,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03484e03ea83451792c4f30071865089","value":1273232}},"86abd68de01346b6bfde05efa8b89a38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23f90d5c5eb442deae1a2e442126434d","placeholder":"​","style":"IPY_MODEL_b5e2fe17194644fab240998d8c6c9993","value":" 1.27M/1.27M [00:00&lt;00:00, 32.6MB/s]"}},"41ef1b39759f4c788f2526eae6deabc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"984f6a7bf00e46d0ae32f3d13360c0cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3c15884dd7f47949b7e9607e0b3d63a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa1bfaf9104444a99c8ebdc5073151d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03484e03ea83451792c4f30071865089":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23f90d5c5eb442deae1a2e442126434d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5e2fe17194644fab240998d8c6c9993":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b391f452dd404c05aa6f7707d6f31b30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f3e024057d14b8794f334dbbfecae98","IPY_MODEL_fc06d17a5eca47a79a8c0215b513a6aa","IPY_MODEL_82ef175bb5b4430a873dbe038bcd44a4"],"layout":"IPY_MODEL_f30cd5b6efed4272aebbeccacdd5cb54"}},"8f3e024057d14b8794f334dbbfecae98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fab8a8fed1448589a1e2a1bf0b6445","placeholder":"​","style":"IPY_MODEL_dd0a57b00743417dab0f47778481a30f","value":"tf_model.h5: 100%"}},"fc06d17a5eca47a79a8c0215b513a6aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c04600b14844bcd861f18f97d30904b","max":298394200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e52319524cf8417f89336c44a81e85b8","value":298394200}},"82ef175bb5b4430a873dbe038bcd44a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8b9e1c1ad744557a0090e45866faf59","placeholder":"​","style":"IPY_MODEL_e089fc50be7c4c678ee84cc9dd0d8add","value":" 298M/298M [00:05&lt;00:00, 30.2MB/s]"}},"f30cd5b6efed4272aebbeccacdd5cb54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7fab8a8fed1448589a1e2a1bf0b6445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0a57b00743417dab0f47778481a30f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c04600b14844bcd861f18f97d30904b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e52319524cf8417f89336c44a81e85b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8b9e1c1ad744557a0090e45866faf59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e089fc50be7c4c678ee84cc9dd0d8add":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"408eae87f44b42849652ef2350fd9570":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46b63c5560164c38bff67de1fa434720","IPY_MODEL_d3fed8c18cbd44cc9e4ea95ebbf898cd","IPY_MODEL_0e7c537603fc42e5a4e7df43c669ed33"],"layout":"IPY_MODEL_53fb4649bd5741d08aa4ae7f124b90bf"}},"46b63c5560164c38bff67de1fa434720":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b7978257d804747bd20fdd1ffe1bd6c","placeholder":"​","style":"IPY_MODEL_dc1336f9302b49219c696ac714faa684","value":"generation_config.json: 100%"}},"d3fed8c18cbd44cc9e4ea95ebbf898cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3327cb29a44507b5f835ab897167be","max":293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5dae53f79b364cfca3f8ea2de502e16e","value":293}},"0e7c537603fc42e5a4e7df43c669ed33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa0113c681764612b43bdd25a7faf0a6","placeholder":"​","style":"IPY_MODEL_13512652e3a94b0791da0c8246a2760d","value":" 293/293 [00:00&lt;00:00, 13.8kB/s]"}},"53fb4649bd5741d08aa4ae7f124b90bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b7978257d804747bd20fdd1ffe1bd6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc1336f9302b49219c696ac714faa684":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e3327cb29a44507b5f835ab897167be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dae53f79b364cfca3f8ea2de502e16e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa0113c681764612b43bdd25a7faf0a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13512652e3a94b0791da0c8246a2760d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **WELCOME TO THE TRANSLATION OPTIONS NOTEBOOK!**\n","\n","In this notebook, we explore different types of translation models and techniques for translating text from one language to another. Below are the items covered in this notebook:\n","\n","0. Recap of the Simple Transformer Model\n","\n","A. English to German Translation using LSTM\n","\n","B. English to German Translation using Custom Transformer Model\n","\n","C. English to German Translation using a Prebuilt Transformer Model\n","\n","Each section of the notebook demonstrates how to implement a translation model for a specific language pair using different architectures and approaches. By exploring these options, you'll gain insights into the strengths and limitations of each translation method and how to choose the most suitable approach for your translation task. Let's dive in!"],"metadata":{"id":"-UwlnIEKqszn"}},{"cell_type":"markdown","source":["#**0. The Simple Transformer Model**"],"metadata":{"id":"ndbVT75iqkXL"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding, Concatenate\n","from tensorflow.keras.models import Model\n","\n","class Transformer(Model):\n","    def __init__(self, vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        # Define embedding layer\n","        self.embedding_layer = Embedding(vocab_size, d_model)\n","\n","        # Define positional encoding layer\n","        self.positional_encoding = self.get_positional_encoding(max_sequence_length, d_model)\n","\n","        # Define transformer layers\n","        self.transformer_layers = [self.create_transformer_layer(d_model, num_heads, dropout_rate) for _ in range(num_layers)]\n","\n","        # Define output layer\n","        self.output_layer = Dense(vocab_size)\n","\n","    def get_positional_encoding(self, max_sequence_length, d_model):\n","        # Calculate positional encodings\n","        positional_encoding = []\n","        for pos in range(max_sequence_length):\n","            pos_encoding = [pos / tf.pow(tf.constant(10000, dtype=tf.float32), 2 * (i // 2) / tf.cast(d_model, tf.float32)) for i in range(d_model)]\n","            if pos % 2 == 0:\n","                positional_encoding.append(tf.math.sin(pos_encoding))\n","            else:\n","                positional_encoding.append(tf.math.cos(pos_encoding))\n","        positional_encoding = tf.stack(positional_encoding)\n","        return tf.expand_dims(positional_encoding, axis=0)\n","\n","    def create_transformer_layer(self, d_model, num_heads, dropout_rate):\n","        # Create transformer layer\n","        return MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads, dropout=dropout_rate)\n","\n","    def call(self, inputs, training=False):\n","        # Define forward pass of the model\n","        input_sequence, target_sequence = inputs\n","\n","        # Embed input sequence and add positional encoding\n","        embedded_input = self.embedding_layer(input_sequence) + self.positional_encoding[:, :tf.shape(input_sequence)[1], :]\n","\n","        # Apply transformer layers sequentially\n","        transformer_output = embedded_input\n","        for layer in self.transformer_layers:\n","            transformer_output = layer(query=transformer_output, value=transformer_output, attention_mask=None, training=training)\n","\n","        # Apply output layer\n","        output_logits = self.output_layer(transformer_output)\n","\n","        return output_logits\n","\n","# Example usage:\n","vocab_size = 10000  # Example vocabulary size\n","max_sequence_length = 50  # Example maximum sequence length\n","d_model = 128  # Example model dimensionality\n","num_heads = 4  # Example number of attention heads\n","num_layers = 2  # Example number of transformer layers\n","dropout_rate = 0.1  # Example dropout rate\n","\n","# Instantiate the Transformer model\n","transformer_model = Transformer(vocab_size, max_sequence_length, d_model, num_heads, num_layers, dropout_rate)\n","\n","# Compile the model\n","transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Define and initialize tokenizer object (replace this with your actual tokenizer)\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n","\n","# Example usage with text data\n","input_text = [\"This is an example sentence\", \"Another example sentence\"]\n","target_text = [\"Dies ist ein Beispiel Satz\", \"Ein weiterer Beispiel Satz\"]\n","\n","# Tokenize input sequences\n","input_sequences = tokenizer.texts_to_sequences(input_text)\n","\n","# Pad sequences to ensure equal length\n","input_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Tokenize target sequences (if applicable)\n","target_sequences = tokenizer.texts_to_sequences(target_text)\n","target_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Define model inputs\n","inputs = (input_sequences_padded, target_sequences_padded)\n","\n","# Feed data into the model\n","output_logits = transformer_model(inputs)\n","\n","# Extract predictions (if applicable)\n","predicted_sequences = tf.argmax(output_logits, axis=-1)\n"],"metadata":{"id":"KEldtdsttiCc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**A. Translator with LSTM**\n","In this completed code:\n","\n","* English and German sentences are tokenized using the Tokenizer class.\n","* The sequences are padded to ensure equal length using the pad_sequences function.\n","* The model architecture is defined with an embedding layer, LSTM layers for both the encoder and decoder, and a dense layer for the decoder output.\n","* The model is compiled with the RMSprop optimizer and sparse categorical cross-entropy loss.\n","* Finally, the model is trained using the fit method with the English and German sequences as input and target, respectively."],"metadata":{"id":"YQiUwDHb9tjh"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","from tensorflow.keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","\n","# Example English and German sentences (replace with your actual data)\n","english_sentences = ['Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.',\n","                     'AI technologies enable machines to perceive their environment, learn from experience, and make decisions to achieve specific goals.',\n","                     'From virtual assistants to self-driving cars, AI applications are revolutionizing various industries and reshaping the way we interact with technology.']\n","spanish_sentences = ['La inteligencia artificial (IA) es la simulación de procesos de inteligencia humana mediante máquinas, especialmente sistemas informáticos.',\n","                    'Las tecnologías de inteligencia artificial permiten a las máquinas percibir su entorno, aprender de la experiencia y tomar decisiones para lograr objetivos específicos.',\n","                    'Desde asistentes virtuales hasta vehículos autónomos, las aplicaciones de IA están revolucionando diversas industrias y remodelando la forma en que interactuamos con la tecnología.']\n","\n","# Tokenize the English sentences\n","english_tokenizer = Tokenizer()\n","english_tokenizer.fit_on_texts(english_sentences)\n","english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n","max_english_length = max(len(seq) for seq in english_sequences)\n","english_vocab_size = len(english_tokenizer.word_index) + 1\n","\n","# Tokenize the German sentences\n","spanish_tokenizer = Tokenizer()\n","spanish_tokenizer.fit_on_texts(spanish_sentences)\n","spanish_sentences = spanish_tokenizer.texts_to_sequences(spanish_sentences)\n","max_german_length = max(len(seq) for seq in spanish_sentences)\n","spanish_vocab_size = len(spanish_tokenizer.word_index) + 1\n","\n","# Pad sequences to ensure equal length\n","english_sequences_padded = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n","german_sequences_padded = pad_sequences(spanish_sentences, maxlen=max_german_length, padding='post')\n","\n","# Define model architecture\n","latent_dim = 256  # Dimensionality of the LSTM layer\n","encoder_inputs = Input(shape=(None,))\n","encoder_embedding = Embedding(input_dim=english_vocab_size, output_dim=latent_dim, mask_zero=True)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(input_dim=spanish_vocab_size, output_dim=latent_dim, mask_zero=True)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","decoder_dense = Dense(spanish_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([english_sequences_padded, german_sequences_padded[:, :-1]], np.expand_dims(german_sequences_padded[:, 1:], -1), batch_size=64, epochs=50, validation_split=0.2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9BqUR7L_lRn","executionInfo":{"status":"ok","timestamp":1713579394339,"user_tz":300,"elapsed":28488,"user":{"displayName":"Pavan Kumar Battula","userId":"16738401156277944723"}},"outputId":"f433bec1-d583-47af-dd2f-a99542760826"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1/1 [==============================] - 13s 13s/step - loss: 3.8919 - val_loss: 3.8956\n","Epoch 2/50\n","1/1 [==============================] - 0s 181ms/step - loss: 3.8694 - val_loss: 3.9007\n","Epoch 3/50\n","1/1 [==============================] - 0s 188ms/step - loss: 3.8495 - val_loss: 3.9065\n","Epoch 4/50\n","1/1 [==============================] - 0s 229ms/step - loss: 3.8278 - val_loss: 3.9147\n","Epoch 5/50\n","1/1 [==============================] - 0s 195ms/step - loss: 3.8007 - val_loss: 3.9297\n","Epoch 6/50\n","1/1 [==============================] - 0s 220ms/step - loss: 3.7603 - val_loss: 3.9700\n","Epoch 7/50\n","1/1 [==============================] - 0s 215ms/step - loss: 3.6834 - val_loss: 4.1700\n","Epoch 8/50\n","1/1 [==============================] - 0s 237ms/step - loss: 3.5301 - val_loss: 4.5853\n","Epoch 9/50\n","1/1 [==============================] - 0s 247ms/step - loss: 3.3833 - val_loss: 4.7919\n","Epoch 10/50\n","1/1 [==============================] - 0s 234ms/step - loss: 3.2597 - val_loss: 5.3688\n","Epoch 11/50\n","1/1 [==============================] - 0s 230ms/step - loss: 3.1217 - val_loss: 5.2972\n","Epoch 12/50\n","1/1 [==============================] - 0s 225ms/step - loss: 2.9849 - val_loss: 4.8180\n","Epoch 13/50\n","1/1 [==============================] - 0s 222ms/step - loss: 3.3717 - val_loss: 5.6769\n","Epoch 14/50\n","1/1 [==============================] - 0s 217ms/step - loss: 2.9340 - val_loss: 5.5842\n","Epoch 15/50\n","1/1 [==============================] - 0s 212ms/step - loss: 2.7765 - val_loss: 5.7349\n","Epoch 16/50\n","1/1 [==============================] - 0s 222ms/step - loss: 2.6694 - val_loss: 5.7731\n","Epoch 17/50\n","1/1 [==============================] - 0s 227ms/step - loss: 2.6045 - val_loss: 6.0234\n","Epoch 18/50\n","1/1 [==============================] - 0s 244ms/step - loss: 2.6326 - val_loss: 5.8469\n","Epoch 19/50\n","1/1 [==============================] - 0s 220ms/step - loss: 2.6913 - val_loss: 6.0645\n","Epoch 20/50\n","1/1 [==============================] - 0s 216ms/step - loss: 2.6731 - val_loss: 5.9340\n","Epoch 21/50\n","1/1 [==============================] - 0s 211ms/step - loss: 2.4464 - val_loss: 6.1528\n","Epoch 22/50\n","1/1 [==============================] - 0s 257ms/step - loss: 2.3681 - val_loss: 6.2638\n","Epoch 23/50\n","1/1 [==============================] - 0s 211ms/step - loss: 2.3157 - val_loss: 6.4011\n","Epoch 24/50\n","1/1 [==============================] - 0s 200ms/step - loss: 2.3397 - val_loss: 6.4860\n","Epoch 25/50\n","1/1 [==============================] - 0s 209ms/step - loss: 2.4681 - val_loss: 6.4279\n","Epoch 26/50\n","1/1 [==============================] - 0s 216ms/step - loss: 2.5911 - val_loss: 6.2700\n","Epoch 27/50\n","1/1 [==============================] - 0s 241ms/step - loss: 2.2254 - val_loss: 6.5086\n","Epoch 28/50\n","1/1 [==============================] - 0s 227ms/step - loss: 2.1460 - val_loss: 6.6601\n","Epoch 29/50\n","1/1 [==============================] - 0s 245ms/step - loss: 2.0932 - val_loss: 6.7576\n","Epoch 30/50\n","1/1 [==============================] - 0s 398ms/step - loss: 2.0974 - val_loss: 6.9231\n","Epoch 31/50\n","1/1 [==============================] - 0s 406ms/step - loss: 2.1801 - val_loss: 6.7278\n","Epoch 32/50\n","1/1 [==============================] - 0s 396ms/step - loss: 2.3486 - val_loss: 6.7778\n","Epoch 33/50\n","1/1 [==============================] - 0s 412ms/step - loss: 2.1268 - val_loss: 6.7396\n","Epoch 34/50\n","1/1 [==============================] - 0s 405ms/step - loss: 2.1681 - val_loss: 6.7232\n","Epoch 35/50\n","1/1 [==============================] - 0s 395ms/step - loss: 1.9380 - val_loss: 6.9215\n","Epoch 36/50\n","1/1 [==============================] - 0s 394ms/step - loss: 1.8741 - val_loss: 7.1727\n","Epoch 37/50\n","1/1 [==============================] - 0s 259ms/step - loss: 1.8330 - val_loss: 7.0260\n","Epoch 38/50\n","1/1 [==============================] - 0s 224ms/step - loss: 1.9059 - val_loss: 7.3329\n","Epoch 39/50\n","1/1 [==============================] - 0s 227ms/step - loss: 1.9058 - val_loss: 7.0227\n","Epoch 40/50\n","1/1 [==============================] - 0s 238ms/step - loss: 2.0148 - val_loss: 7.0446\n","Epoch 41/50\n","1/1 [==============================] - 0s 245ms/step - loss: 1.7251 - val_loss: 7.3464\n","Epoch 42/50\n","1/1 [==============================] - 0s 216ms/step - loss: 1.6480 - val_loss: 7.3521\n","Epoch 43/50\n","1/1 [==============================] - 0s 219ms/step - loss: 1.5963 - val_loss: 7.6246\n","Epoch 44/50\n","1/1 [==============================] - 0s 234ms/step - loss: 1.6121 - val_loss: 7.5849\n","Epoch 45/50\n","1/1 [==============================] - 0s 223ms/step - loss: 1.6323 - val_loss: 7.6375\n","Epoch 46/50\n","1/1 [==============================] - 0s 259ms/step - loss: 1.6620 - val_loss: 7.5619\n","Epoch 47/50\n","1/1 [==============================] - 0s 246ms/step - loss: 1.5017 - val_loss: 7.4804\n","Epoch 48/50\n","1/1 [==============================] - 0s 263ms/step - loss: 1.4485 - val_loss: 8.0008\n","Epoch 49/50\n","1/1 [==============================] - 0s 242ms/step - loss: 1.5115 - val_loss: 7.3694\n","Epoch 50/50\n","1/1 [==============================] - 0s 243ms/step - loss: 1.7425 - val_loss: 7.3577\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7c49684d3460>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Example English sentences to translate (replace with your test data)\n","english_test_sentences = ['We are happy', 'They love ice cream']\n","\n","# Tokenize and pad test data\n","english_test_sequences = english_tokenizer.texts_to_sequences(english_test_sentences)\n","english_test_sequences_padded = pad_sequences(english_test_sequences, maxlen=max_english_length, padding='post')\n","\n","# Predict German translations\n","predicted_sequences = model.predict([english_test_sequences_padded, np.zeros((len(english_test_sentences), max_german_length))])\n","\n","# Decode predicted sequences into words\n","predicted_sentences = []\n","for sequence in predicted_sequences:\n","    predicted_sentence = ''\n","    for token in sequence:\n","        predicted_word = german_tokenizer.index_word[np.argmax(token)]\n","        if predicted_word == '<end>':\n","            break\n","        predicted_sentence += predicted_word + ' '\n","    predicted_sentences.append(predicted_sentence.strip())\n","\n","# Print the translations\n","for english_sentence, german_translation in zip(english_test_sentences, predicted_sentences):\n","    print('English:', english_sentence)\n","    print('German:', german_translation)\n","    print()\n"],"metadata":{"id":"xiLQup6HAf7H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713580388780,"user_tz":300,"elapsed":4632,"user":{"displayName":"Pavan Kumar Battula","userId":"16738401156277944723"}},"outputId":"ffe07bec-6578-436c-811c-41827aa2bb7f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 4s 4s/step\n","English: We are happy\n","German: liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest\n","\n","English: They love ice cream\n","German: liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest liest\n","\n"]}]},{"cell_type":"markdown","source":["#**B. Translator with Transformer: Option 1**"],"metadata":{"id":"6vszEx96dZy2"}},{"cell_type":"code","source":["# Import the needed libraries\n","import tensorflow as tf\n","from tensorflow.keras.layers import LayerNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","import numpy as np"],"metadata":{"id":"iOVC9im1lu--"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**1. Encoder Layer**\n","The class TransformerEncoderLayer defines a single layer within the encoder of a Transformer model. In a Transformer architecture, the encoder is responsible for processing input sequences and extracting their representations. Each layer in the encoder consists of two main components: a multi-head self-attention mechanism and a position-wise feedforward neural network (FFNN).\n","\n","Here's a breakdown of what the class does:\n","\n","1. Initialization: The constructor initializes the layer, taking parameters such as the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), and the dropout rate (rate).\n","\n","2. Components: Inside the layer, there are three main components:\n","\n","* Multi-Head Self-Attention (MHA): This component allows the model to focus on different parts of the input sequence simultaneously. It takes the input sequence, computes attention weights for each position, and combines information from different positions based on these weights.\n","* Feedforward Neural Network (FFNN): After the attention mechanism, the output passes through a position-wise feedforward neural network. This network applies a series of linear transformations and non-linear activations to each position independently.\n","* Layer Normalization and Dropout: Layer normalization and dropout are applied after each sub-layer (attention and feedforward network) to stabilize training and prevent overfitting.\n","3. Call Method: The call method defines how the layer processes input data. It takes the input sequence and a boolean flag indicating whether the model is in training mode. Inside the method, the input sequence is passed through the multi-head self-attention mechanism and feedforward neural network in sequence. Layer normalization and dropout are applied after each sub-layer, and the output of the layer is returned.\n","\n","In summary, the TransformerEncoderLayer class encapsulates the functionality of a single layer within the encoder of a Transformer model. It performs operations such as multi-head self-attention and position-wise feedforward processing on input sequences, facilitating the extraction of meaningful representations from the data."],"metadata":{"id":"_x5MPLjTi3zD"}},{"cell_type":"code","source":["# Define the TransformerEncoderLayer and TransformerDecoderLayer classes\n","class TransformerEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","\n","        # Multi-head self-attention mechanism\n","        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","        # Feedforward neural network\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation='relu'),\n","            tf.keras.layers.Dense(d_model)\n","        ])\n","\n","        # Layer normalization\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","\n","        # Dropout layers\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        # Self-attention mechanism\n","        attn_output = self.mha(inputs, inputs, attention_mask=None, training=training)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","\n","        # Feedforward network\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n"],"metadata":{"id":"EvgDaLROjQYR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**2. Decoder Layer**\n","The TransformerDecoderLayer class defines a single layer within the decoder of a Transformer model. In the Transformer architecture, the decoder is responsible for generating output sequences based on the representations learned by the encoder. Each layer in the decoder consists of three main components: multi-head self-attention, encoder-decoder attention, and a position-wise feedforward neural network (FFNN).\n","\n","Here's an explanation of what the class does:\n","\n","1. Initialization: The constructor initializes the layer, taking parameters such as the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), and the dropout rate (rate).\n","\n","2. Components: Inside the layer, there are three main components:\n","\n","* Masked Multi-Head Self-Attention (MHA): This component allows the decoder to attend to previous positions in the output sequence while preventing it from attending to future positions. It computes attention weights for each position in the output sequence and combines information from different positions based on these weights.\n","* Encoder-Decoder Attention: This component allows the decoder to take into account the representations learned by the encoder. It computes attention weights between the current position in the output sequence and the input sequence representations, enabling the decoder to align its output with the input.\n","* Feedforward Neural Network (FFNN): After the attention mechanisms, the output passes through a position-wise feedforward neural network, similar to the encoder. This network applies a series of linear transformations and non-linear activations to each position independently.\n","* Layer Normalization and Dropout: Layer normalization and dropout are applied after each sub-layer (masked self-attention, encoder-decoder attention, and feedforward network) to stabilize training and prevent overfitting.\n","3. Call Method: The call method defines how the layer processes input data. It takes the input sequence, encoder output, and a boolean flag indicating whether the model is in training mode. Inside the method, the input sequence is passed through the masked multi-head self-attention mechanism, encoder-decoder attention mechanism, and feedforward neural network in sequence. Layer normalization and dropout are applied after each sub-layer, and the output of the layer is returned."],"metadata":{"id":"8rXdOMeAlZ5j"}},{"cell_type":"code","source":["class TransformerDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerDecoderLayer, self).__init__()\n","\n","        # Multi-head attention mechanisms\n","        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","\n","        # Feedforward neural network\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation='relu'),\n","            tf.keras.layers.Dense(d_model)\n","        ])\n","\n","        # Layer normalization\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","\n","        # Dropout layers\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, enc_output, training):\n","        # Self-attention mechanism\n","        attn1 = self.mha1(inputs, inputs, attention_mask=None, training=training)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + inputs)\n","\n","        # Encoder-decoder attention mechanism\n","        attn2 = self.mha2(out1, enc_output, attention_mask=None, training=training)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        # Feedforward network\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3\n"],"metadata":{"id":"kkM5zMwCjr3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**3. Implement the Translator**\n","The TransformerTranslator class is a custom Keras model that implements a sequence-to-sequence transformer for translation tasks, such as English to German translation. Here's what the class does:\n","\n","1. Initialization: The constructor initializes the model, taking parameters such as the number of layers (num_layers), the dimensionality of the model (d_model), the number of attention heads (num_heads), the dimensionality of the feedforward neural network (dff), the vocabulary size of the input and target languages (input_vocab_size, target_vocab_size), and the dropout rate (dropout_rate).\n","\n","2. Components: Inside the model, there are two main components:\n","\n","* Encoder: The encoder consists of a stack of transformer encoder layers. Each layer contains multi-head self-attention and feedforward neural network sub-layers. The encoder processes the input sequences (English sentences) and generates representations for each token in the input sequence.\n","* Decoder: The decoder consists of a stack of transformer decoder layers. Each layer contains masked multi-head self-attention, encoder-decoder attention, and feedforward neural network sub-layers. The decoder takes the encoder output and generates output sequences (German translations) based on the learned representations.\n","3. Forward Pass: The call method defines the forward pass of the model. Given input sequences in both the source (English) and target (German) languages, the model passes the input through the encoder to generate encoder output representations. The decoder then takes these representations and generates output sequences in the target language. The final layer of the decoder outputs the predicted token probabilities for each position in the output sequence.\n","4. Loss Calculation: During training, the model calculates the loss between the predicted token probabilities and the actual target tokens using a loss function such as sparse categorical cross-entropy. This loss is used to update the model's weights during optimization.\n","5. Model Compilation: After defining the forward pass and loss calculation, the model is compiled using an optimizer (e.g., Adam) and a loss function. This step prepares the model for training.\n","6. Model Summary: Finally, the model's architecture is summarized using the summary method, which displays the layer names, output shapes, and number of parameters in the model.\n","\n","In summary, the TransformerTranslator class encapsulates the functionality of a sequence-to-sequence transformer model for translation tasks. It combines encoder and decoder components to process input sequences and generate output sequences, facilitating tasks such as language translation."],"metadata":{"id":"BGpOglVZmk0z"}},{"cell_type":"code","source":["# Define the TransformerTranslator model\n","class TransformerTranslator(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","        super(TransformerTranslator, self).__init__()\n","\n","        # Embedding layers for input and target vocabularies\n","        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","\n","        # Transformer encoder and decoder layers\n","        self.transformer_encoder_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate)\n","                                           for _ in range(num_layers)]\n","        self.transformer_decoder_layers = [TransformerDecoderLayer(d_model, num_heads, dff, dropout_rate)\n","                                           for _ in range(num_layers)]\n","\n","        # Final output layer\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inputs, targets, training):\n","        # Encoder padding mask\n","        enc_padding_mask = None\n","        # Decoder padding mask\n","        dec_padding_mask = None\n","\n","        # Encoder\n","        encoder_input = self.encoder_embedding(inputs)\n","        for layer in self.transformer_encoder_layers:\n","            encoder_input = layer(encoder_input, training)\n","\n","        # Decoder\n","        decoder_input = self.decoder_embedding(targets)\n","        for layer in self.transformer_decoder_layers:\n","            decoder_input = layer(decoder_input, encoder_input, training)\n","\n","        # Final output\n","        output = self.final_layer(decoder_input)\n","        return output\n"],"metadata":{"id":"ykC0KbGQmU9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**4. Configure Transformer Architecture**\n","The example hyperparameters define various aspects of the transformer architecture and training process. Here's a breakdown of what each hyperparameter does:\n","\n","* num_layers: This hyperparameter determines the number of encoder and decoder layers in the transformer model. Each layer contains multiple sub-layers, such as multi-head self-attention and feedforward neural networks.\n","\n","* d_model: The dimensionality of the model, also known as the hidden size. It represents the dimensionality of the embedding space and the internal representations within the transformer layers.\n","\n","* num_heads: The number of attention heads in the multi-head attention mechanism. More attention heads allow the model to focus on different parts of the input sequence simultaneously, capturing more complex relationships.\n","\n","* dff: The dimensionality of the feedforward neural network layer within each transformer layer. It determines the size of the hidden layer in the feedforward network.\n","\n","* input_vocab_size: The size of the vocabulary for the input language (e.g., English). It represents the total number of unique tokens or words in the vocabulary.\n","\n","* target_vocab_size: The size of the vocabulary for the target language (e.g., German). Similar to the input vocabulary size, it represents the total number of unique tokens or words in the target language vocabulary.\n","\n","* dropout_rate: The dropout rate applied to the outputs of each transformer layer during training. Dropout is a regularization technique that helps prevent overfitting by randomly dropping units (along with their connections) from the network during training.\n","\n","These hyperparameters collectively define the architecture and behavior of the transformer model, influencing its capacity, attention mechanism, and regularization during training. Adjusting these hyperparameters allows practitioners to tailor the model to specific tasks and datasets, balancing model complexity and performance."],"metadata":{"id":"OjH3LR2FnP8i"}},{"cell_type":"code","source":["# Example hyperparameters\n","num_layers = 4\n","d_model = 128\n","num_heads = 8\n","dff = 512\n","input_vocab_size = 10000\n","target_vocab_size = 10000\n","dropout_rate = 0.1"],"metadata":{"id":"UChrnEGonGYl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**5. Initialize and Compile the Model**\n","The \"Initialize and compile the model\" section of the code involves setting up the model architecture and configuring its training process. Here's what each step in this section does:\n","\n","* Model Initialization: The TransformerTranslator class is instantiated with the specified hyperparameters (num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate). This creates an instance of the transformer model with the specified architecture.\n","\n","* Optimizer Initialization: An optimizer is chosen to update the model parameters during training. In this case, the Adam optimizer is used, which is a popular optimization algorithm for deep learning models. It adapts the learning rate for each parameter during training.\n","\n","* Loss Function Initialization: The loss function is defined to measure the discrepancy between the model predictions and the actual target values. In this example, SparseCategoricalCrossentropy is used as the loss function. It is suitable for classification tasks with integer targets and sparse target values.\n","\n","* Model Compilation: The model is compiled with the optimizer and loss function. This step configures the model for training by specifying the optimization algorithm and the loss function to be minimized. Additionally, any additional metrics can be specified here to monitor during training.\n","\n","Overall, this section prepares the model for training by specifying its architecture, optimization algorithm, loss function, and any additional metrics to be tracked. After compilation, the model is ready to be trained on the training data."],"metadata":{"id":"x9DAR8DEnrmJ"}},{"cell_type":"code","source":["# Initialize and compile the model\n","model = TransformerTranslator(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n","optimizer = Adam()\n","loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","# Define custom loss function\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n","\n","model.compile(optimizer=optimizer, loss=loss_function)"],"metadata":{"id":"bzTotF2Dnjfe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**6. Model Summary**\n","The \"Display model summary\" section of the code generates a summary of the model architecture, providing useful information about the different layers and parameters in the model. Here's what each step in this section does:\n","\n","1. Model Summary: The summary() method is called on the model object. This method prints a concise summary of the model architecture to the console. It includes information such as:\n","* The type of each layer in the model.\n","* The output shape of each layer.\n","* The number of parameters in each layer.\n","* The total number of parameters in the model.\n","By displaying the model summary, you can quickly inspect the architecture of the model, including the number of layers, the shape of the input and output tensors, and the number of trainable parameters. This information is helpful for debugging, optimizing the model, and ensuring that the architecture is as intended.\n","\n","\n"],"metadata":{"id":"xjzoqvXpoDuo"}},{"cell_type":"code","source":["# Display model summary\n","input_example = tf.random.uniform((64, 50), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n","output_example = tf.random.uniform((64, 50), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n","encoder_padding_mask = None\n","decoder_padding_mask = None\n","\n","model(input_example, output_example, training=False)\n","model.summary()"],"metadata":{"id":"T1He4TiQoDMu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**THE ENTIRE CODE**"],"metadata":{"id":"wCKyGkNIpeER"}},{"cell_type":"code","source":["# ****************** THE ENTIRE CODE *******************\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import LayerNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","import numpy as np\n","\n","# Define the TransformerEncoderLayer and TransformerDecoderLayer classes\n","class TransformerEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","\n","        # Multi-head self-attention mechanism\n","        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","        # Feedforward neural network\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation='relu'),\n","            tf.keras.layers.Dense(d_model)\n","        ])\n","\n","        # Layer normalization\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","\n","        # Dropout layers\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        # Self-attention mechanism\n","        attn_output = self.mha(inputs, inputs, attention_mask=None, training=training)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","\n","        # Feedforward network\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n","\n","class TransformerDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerDecoderLayer, self).__init__()\n","\n","        # Multi-head attention mechanisms\n","        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","\n","        # Feedforward neural network\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation='relu'),\n","            tf.keras.layers.Dense(d_model)\n","        ])\n","\n","        # Layer normalization\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","\n","        # Dropout layers\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, enc_output, training):\n","        # Self-attention mechanism\n","        attn1 = self.mha1(inputs, inputs, attention_mask=None, training=training)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + inputs)\n","\n","        # Encoder-decoder attention mechanism\n","        attn2 = self.mha2(out1, enc_output, attention_mask=None, training=training)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        # Feedforward network\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3\n","\n","# Define the TransformerTranslator model\n","class TransformerTranslator(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","        super(TransformerTranslator, self).__init__()\n","\n","        # Embedding layers for input and target vocabularies\n","        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","\n","        # Transformer encoder and decoder layers\n","        self.transformer_encoder_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate)\n","                                           for _ in range(num_layers)]\n","        self.transformer_decoder_layers = [TransformerDecoderLayer(d_model, num_heads, dff, dropout_rate)\n","                                           for _ in range(num_layers)]\n","\n","        # Final output layer\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inputs, targets, training):\n","        # Encoder padding mask\n","        enc_padding_mask = None\n","        # Decoder padding mask\n","        dec_padding_mask = None\n","\n","        # Encoder\n","        encoder_input = self.encoder_embedding(inputs)\n","        for layer in self.transformer_encoder_layers:\n","            encoder_input = layer(encoder_input, training)\n","\n","        # Decoder\n","        decoder_input = self.decoder_embedding(targets)\n","        for layer in self.transformer_decoder_layers:\n","            decoder_input = layer(decoder_input, encoder_input, training)\n","\n","        # Final output\n","        output = self.final_layer(decoder_input)\n","        return output\n","\n","# Example hyperparameters\n","num_layers = 4\n","d_model = 128\n","num_heads = 8\n","dff = 512\n","input_vocab_size = 10000\n","target_vocab_size = 10000\n","dropout_rate = 0.1\n","\n","# Initialize and compile the model\n","model = TransformerTranslator(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n","optimizer = Adam()\n","loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","# Define custom loss function\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n","\n","model.compile(optimizer=optimizer, loss=loss_function)\n","\n","# Display model summary\n","input_example = tf.random.uniform((64, 50), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n","output_example = tf.random.uniform((64, 50), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n","encoder_padding_mask = None\n","decoder_padding_mask = None\n","\n","model(input_example, output_example, training=False)\n","model.summary()\n"],"metadata":{"id":"-S762-xxhfBp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**C. Translator with Transformer: Option 2**\n","\n","The code below uses the transformers library to load a pre-trained translation model (**opus-mt-en-de**) and its tokenizer. Then, it translates English sentences to German using the model's generate method. Finally, it prints the translations.\n","\n","The **opus-mt-en-de** library is a machine translation (MT) model trained specifically for translating text from English to German. It utilizes the OPUS-MT framework, which is an open-source initiative that provides pre-trained neural machine translation models for various language pairs. The opus-mt-en-de model is trained on a large corpus of English-German parallel text data and is capable of translating English sentences into German with high accuracy. This library makes it easy to perform English to German translation tasks programmatically by providing a ready-to-use translation model.\n","\n","Please make sure you have the transformers library installed (pip install transformers) before running this code."],"metadata":{"id":"VgMLruc79zP3"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n","\n","# Load pre-trained model and tokenizer\n","model_name = \"Helsinki-NLP/opus-mt-en-de\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","# Translate English sentences to German\n","english_sentences = ['I am a student', 'He likes apples', 'She is reading a book']\n","german_translations = []\n","\n","for sentence in english_sentences:\n","    input_ids = tokenizer.encode(sentence, return_tensors=\"tf\", padding=True)\n","    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n","    german_translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    german_translations.append(german_translation)\n","\n","# Print translations\n","for english_sentence, german_translation in zip(english_sentences, german_translations):\n","    print('English:', english_sentence)\n","    print('German:', german_translation)\n","    print()\n"],"metadata":{"id":"RstFanr2cukk","colab":{"base_uri":"https://localhost:8080/","height":625,"referenced_widgets":["099b7136b2854d45ad187a816128b362","aad877fde22d4c08a4c2c9fa50804632","a353bed2d24c4e7c9e64c5525f2f3f77","98c8ff5eaa5749ecb23ee72a31cbc99e","c505e3d30c70412ba85f4a72ba83cada","545057a15ec3430d979489085ba1054c","440188a6ff1242b5af73d52a78f8df30","c21f2b98956b4a54abaf43c31c526c38","fb71a238c94c4249af4836bac9e22428","b1186273f45b42e4be7150fcbf612925","0311eb2a2fd740e2bed1c657e4025921","47bbeb11ea774c33b174860f6f9405e3","104f8075ece242eea62bbec2f2104244","d02ca3f5200042fbb452cc7eda7aa4ea","2c16e3bdbe804ef48174a2cc2643cb80","84dbb785b5de48899bab94f99bd6e4b1","e7a48076a6cf4d61bad8ef8a59fea1cd","3bc7aaf8648a4214b3ac68e9f9e579bb","44a28cd7579045de9219ed1deec93430","eb50c59e9c6d41ab90c660c57f376a44","64757bde8c3241e495ad1ceb000e4047","80b85eee1f04460bb6d6492806ed87c2","35bb014fa8684dcb997c9f4e54776a51","c85a2626012047dcb09f37cf39308f5d","e850e77136cb4af5ab0b3db779bfd7eb","0619bd2933684e8e9d596068492d60f6","c54155ecd6e44beba644ecbddd735565","95ec91753c964ba5b23c20e345db115a","55dbd5c87cd84d368ab1f3e393993b52","52f284be904f4b24bbe34b911746d6c4","3fb6bc3282d3454c8e2c171ea10601ad","72f5773b4c86454f84fc2f94cc4b15e5","8b986f79fda04c469ff9c31b37f358d5","f6d0871939a7467f8982f555f9b7b32a","e206bba1cc4048a3a5989162cbdac97c","8e1abf4611d54974979f80b792089894","b0f7486da08444b1a9771a3e99bd55e1","8039cea9967c48948e964c3e726738dc","72690da867544b6d968d9e9a0bdd6f15","c7b197ff3265460baad8f08a430fb451","a199e90bc50843b0903d600a9480b2e1","7284cf3e6a424ecebb6beb46040fd38a","6c694dc7c6ce4375973fd1c93c7f5ad0","3b97da80d52a45adaccf7b12fd4f7151","f468c3f84b6e4c2a91df36e72bb81eb1","088f72f0c35b43f9834b29d48bf631dc","53c6bcee810b48eeb8291c1e717b44ab","86abd68de01346b6bfde05efa8b89a38","41ef1b39759f4c788f2526eae6deabc0","984f6a7bf00e46d0ae32f3d13360c0cd","e3c15884dd7f47949b7e9607e0b3d63a","aa1bfaf9104444a99c8ebdc5073151d2","03484e03ea83451792c4f30071865089","23f90d5c5eb442deae1a2e442126434d","b5e2fe17194644fab240998d8c6c9993","b391f452dd404c05aa6f7707d6f31b30","8f3e024057d14b8794f334dbbfecae98","fc06d17a5eca47a79a8c0215b513a6aa","82ef175bb5b4430a873dbe038bcd44a4","f30cd5b6efed4272aebbeccacdd5cb54","c7fab8a8fed1448589a1e2a1bf0b6445","dd0a57b00743417dab0f47778481a30f","3c04600b14844bcd861f18f97d30904b","e52319524cf8417f89336c44a81e85b8","d8b9e1c1ad744557a0090e45866faf59","e089fc50be7c4c678ee84cc9dd0d8add","408eae87f44b42849652ef2350fd9570","46b63c5560164c38bff67de1fa434720","d3fed8c18cbd44cc9e4ea95ebbf898cd","0e7c537603fc42e5a4e7df43c669ed33","53fb4649bd5741d08aa4ae7f124b90bf","3b7978257d804747bd20fdd1ffe1bd6c","dc1336f9302b49219c696ac714faa684","7e3327cb29a44507b5f835ab897167be","5dae53f79b364cfca3f8ea2de502e16e","fa0113c681764612b43bdd25a7faf0a6","13512652e3a94b0791da0c8246a2760d"]},"executionInfo":{"status":"ok","timestamp":1713580488961,"user_tz":300,"elapsed":30264,"user":{"displayName":"Pavan Kumar Battula","userId":"16738401156277944723"}},"outputId":"9c4ed02e-9cb0-48d5-bce0-f529ed012945"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099b7136b2854d45ad187a816128b362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47bbeb11ea774c33b174860f6f9405e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35bb014fa8684dcb997c9f4e54776a51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6d0871939a7467f8982f555f9b7b32a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f468c3f84b6e4c2a91df36e72bb81eb1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"display_data","data":{"text/plain":["tf_model.h5:   0%|          | 0.00/298M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b391f452dd404c05aa6f7707d6f31b30"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMarianMTModel.\n","\n","All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408eae87f44b42849652ef2350fd9570"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["English: I am a student\n","German: Ich bin Studentin\n","\n","English: He likes apples\n","German: Er mag Äpfel\n","\n","English: She is reading a book\n","German: Sie liest ein Buch\n","\n"]}]}]}